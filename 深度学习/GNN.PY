import pandas as pd
import numpy as np
import torch
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, BatchNorm
from sklearn.neighbors import BallTree
from sklearn.preprocessing import StandardScaler
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
import os
try:
    import shap
    _shap_available = True
except Exception:
    _shap_available = False

# =========================
# 1. è¯»å–æ•°æ®
# =========================
csv_file = "regession\play2019_0906.csv"  # ä½ çš„CSVè·¯å¾„
df = pd.read_csv(csv_file)

# ç‰¹å¾åˆ—ï¼ˆå½¢æŒ‡æ ‡ï¼‰
shape_features = [
    "building_ratio0", "railway_ratio1", "road_ratio2", "water_ratio3",
    "instru_ratio4", "green_ratio5", "square_ratio6", "undevelop_ratio7",
    "playground_ratio8", "LPI_Max", "PR_COUNTclass", "SHI_zhouchang",
    "RND_road", "Social", "Life", "Entertainment",
    "Production", "POI_density", "MAX_Elevation", "MEAN_Elevation",
]

# ç›®æ ‡åˆ—ï¼ˆæµå…¥ã€æµå‡ºã€betweennessï¼‰
target_cols = ["PLAY_D_IN", "PLAY_O_OUT", "PLAY__Betweenness"]

coords = df[["lon", "lat"]].values
X = df[shape_features].values
Y = df[target_cols].values

# =========================
# 2. æ•°æ®é¢„å¤„ç† - è‡ªå˜é‡å·²ç»æ ‡å‡†åŒ–ï¼Œç›´æ¥ä½¿ç”¨
# =========================
# è‡ªå˜é‡å·²ç»æ ‡å‡†åŒ–ï¼Œç›´æ¥ä½¿ç”¨
X_processed = X.copy()
# ä¸å¯¹ç›®æ ‡å˜é‡è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä¿æŒåŸå§‹å€¼
Y_original = Y.copy()

# =========================
# 3. å»ºå›¾ï¼ˆç©ºé—´é‚»å±… + è¾¹æƒï¼‰
# =========================
tree = BallTree(np.deg2rad(coords), metric='haversine')
radius = 2.0 / 6371.0  # åŠå¾„ 0.5km (åœ°çƒåŠå¾„å•ä½)

edges = []
edge_weights = []
for i, coord in enumerate(np.deg2rad(coords)):
    idx = tree.query_radius([coord], r=radius)[0]
    for j in idx:
        if i != j:
            dist = np.linalg.norm(coords[i] - coords[j])
            edges.append((i, j))
            edge_weights.append(1 / (dist + 1e-6))  # è·ç¦»å€’æ•°åšæƒé‡

edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
edge_attr = torch.tensor(edge_weights, dtype=torch.float)

# =========================
# 4. æ„å»º PyG æ•°æ®å¯¹è±¡
# =========================
data = Data(
    x=torch.tensor(X_processed, dtype=torch.float),
    edge_index=edge_index,
    edge_attr=edge_attr,
    y=torch.tensor(Y_original, dtype=torch.float)
)

# =========================
# 5. å®šä¹‰æ”¹è¿›çš„ GNN æ¨¡å‹
# =========================
class ImprovedGCN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.3):
        super(ImprovedGCN, self).__init__()
        
        # ç¬¬ä¸€å±‚GCN
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.bn1 = BatchNorm(hidden_channels)
        
        # ç¬¬äºŒå±‚GCN
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.bn2 = BatchNorm(hidden_channels)
        
        # ç¬¬ä¸‰å±‚GCN
        self.conv3 = GCNConv(hidden_channels, hidden_channels // 2)
        self.bn3 = BatchNorm(hidden_channels // 2)
        
        # ç¬¬å››å±‚GCN
        self.conv4 = GCNConv(hidden_channels // 2, hidden_channels // 4)
        self.bn4 = BatchNorm(hidden_channels // 4)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = torch.nn.Linear(hidden_channels // 4, hidden_channels // 2)
        self.fc2 = torch.nn.Linear(hidden_channels // 2, hidden_channels // 4)
        self.fc3 = torch.nn.Linear(hidden_channels // 4, out_channels)
        
        # Dropout
        self.dropout = torch.nn.Dropout(dropout)
        
        # è¾“å‡ºæ¿€æ´»å‡½æ•°ç¡®ä¿éè´Ÿ
        self.relu = torch.nn.ReLU()

    def forward(self, x, edge_index):
        # ç¬¬ä¸€å±‚
        x = self.conv1(x, edge_index)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # ç¬¬äºŒå±‚
        x = self.conv2(x, edge_index)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # ç¬¬ä¸‰å±‚
        x = self.conv3(x, edge_index)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # ç¬¬å››å±‚
        x = self.conv4(x, edge_index)
        x = self.bn4(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # å…¨è¿æ¥å±‚
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        x = self.fc2(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        x = self.fc3(x)
        
        # ä½¿ç”¨ReLUç¡®ä¿è¾“å‡ºéè´Ÿ
        return self.relu(x)

# å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨
class CosineAnnealingScheduler:
    def __init__(self, optimizer, T_max, eta_min=0):
        self.optimizer = optimizer
        self.T_max = T_max
        self.eta_min = eta_min
        self.base_lr = optimizer.param_groups[0]['lr']
        
    def step(self, epoch):
        lr = self.eta_min + (self.base_lr - self.eta_min) * (1 + np.cos(np.pi * epoch / self.T_max)) / 2
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

# å®šä¹‰è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼ˆç»“åˆMSEå’ŒHuberæŸå¤±ï¼‰
def combined_loss(pred, target, alpha=0.7):
    """
    ç»“åˆMSEå’ŒHuberæŸå¤±ï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’
    """
    mse_loss = F.mse_loss(pred, target)
    huber_loss = F.smooth_l1_loss(pred, target)
    return alpha * mse_loss + (1 - alpha) * huber_loss

# åˆ›å»ºæ¨¡å‹
model = ImprovedGCN(in_channels=X.shape[1], hidden_channels=64, out_channels=3, dropout=0.2)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.002, weight_decay=1e-5)
scheduler = CosineAnnealingScheduler(optimizer, T_max=3000, eta_min=1e-6)
loss_fn = combined_loss

# =========================
# 6. è®­ç»ƒ
# =========================
print("å¼€å§‹è®­ç»ƒæ”¹è¿›çš„GNNæ¨¡å‹...")
best_loss = float('inf')
patience = 50
patience_counter = 0

for epoch in range(500):
    model.train()
    optimizer.zero_grad()
    pred = model(data.x, data.edge_index)
    loss = loss_fn(pred, data.y)
    loss.backward()
    
    # æ¢¯åº¦è£å‰ª
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    optimizer.step()
    scheduler.step(epoch)
    
    # æ—©åœæœºåˆ¶
    if loss.item() < best_loss:
        best_loss = loss.item()
        patience_counter = 0
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        torch.save(model.state_dict(), 'best_gnn_model.pth')
    else:
        patience_counter += 1
    
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}")
    
    # æ—©åœ
    if patience_counter >= patience:
        print(f"æ—©åœåœ¨ç¬¬ {epoch} è½®")
        break

# åŠ è½½æœ€ä½³æ¨¡å‹
model.load_state_dict(torch.load('best_gnn_model.pth'))

# =========================
# 7. é¢„æµ‹
# =========================
model.eval()
with torch.no_grad():
    pred = model(data.x, data.edge_index).numpy()

df["pred_in"], df["pred_out"], df["pred_between"] = pred[:, 0], pred[:, 1], pred[:, 2]

# æ£€æŸ¥é¢„æµ‹å€¼çš„èŒƒå›´
print(f"\né¢„æµ‹ç»“æœ:")
print(f"é¢„æµ‹æµå…¥æœ€å°å€¼: {pred[:, 0].min():.2f}, æœ€å¤§å€¼: {pred[:, 0].max():.2f}")
print(f"é¢„æµ‹æµå‡ºæœ€å°å€¼: {pred[:, 1].min():.2f}, æœ€å¤§å€¼: {pred[:, 1].max():.2f}")
print(f"é¢„æµ‹betweennessæœ€å°å€¼: {pred[:, 2].min():.2f}, æœ€å¤§å€¼: {pred[:, 2].max():.2f}")

df.to_csv("regession/output/play2019_0906.csv", index=False)
print("âœ… æ”¹è¿›æ¨¡å‹çš„é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° play2019_0906.csv")

# =========================
# åˆ›å»ºæ ¼ç½‘åŸºç¡€ç»“æœè¡¨
# =========================
print("\nåˆ›å»ºæ ¼ç½‘åŸºç¡€ç»“æœè¡¨...")

# åˆ›å»ºæ ¼ç½‘åŸºç¡€ç»“æœè¡¨
grid_results_data = []

for i in range(len(df)):
    grid_data = {
        'id': df.iloc[i]['id2019'],  # ä½¿ç”¨åŸå§‹æ•°æ®ä¸­çš„id2019ä½œä¸ºæ ¼ç½‘ç¼–å·
        'lon': df.iloc[i]['lon'],  # ç»åº¦åæ ‡
        'lat': df.iloc[i]['lat']   # çº¬åº¦åæ ‡
    }
    
    # æ·»åŠ æ‰€æœ‰å½¢æ€ç‰¹å¾å€¼
    for feature_name in shape_features:
        grid_data[feature_name] = df.iloc[i][feature_name]
    
    # æ·»åŠ ç›®æ ‡å˜é‡ï¼ˆå¯é€‰ï¼‰
    for target_name in target_cols:
        grid_data[target_name] = df.iloc[i][target_name]
    
    # æ·»åŠ é¢„æµ‹å€¼ï¼ˆå¯é€‰ï¼‰
    grid_data['pred_in'] = df.iloc[i]['pred_in']
    grid_data['pred_out'] = df.iloc[i]['pred_out']
    grid_data['pred_between'] = df.iloc[i]['pred_between']
    
    grid_results_data.append(grid_data)

# è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
grid_results_df = pd.DataFrame(grid_results_data)
grid_results_file = "regession/output/grid_results.csv"

# å°è¯•ä¿å­˜æ–‡ä»¶ï¼Œå¦‚æœæƒé™è¢«æ‹’ç»åˆ™ä½¿ç”¨å¤‡ç”¨æ–‡ä»¶å
try:
    grid_results_df.to_csv(grid_results_file, index=False)
    print(f"âœ… æ ¼ç½‘åŸºç¡€ç»“æœè¡¨å·²ä¿å­˜åˆ°: {grid_results_file}")
except PermissionError:
    # å¦‚æœæƒé™è¢«æ‹’ç»ï¼Œä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„å¤‡ç”¨æ–‡ä»¶å
    import datetime
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = f"regession/output/grid_results_{timestamp}.csv"
    grid_results_df.to_csv(backup_file, index=False)
    print(f"âš ï¸  åŸæ–‡ä»¶è¢«å ç”¨ï¼Œå·²ä¿å­˜åˆ°å¤‡ç”¨æ–‡ä»¶: {backup_file}")
    grid_results_file = backup_file
except Exception as e:
    print(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    # å°è¯•ä¿å­˜åˆ°å½“å‰ç›®å½•
    fallback_file = "grid_results_fallback.csv"
    grid_results_df.to_csv(fallback_file, index=False)
    print(f"ğŸ“ å·²ä¿å­˜åˆ°å½“å‰ç›®å½•: {fallback_file}")
    grid_results_file = fallback_file

print(f"   åŒ…å« {len(grid_results_df)} ä¸ªæ ¼ç½‘")
print(f"   åŒ…å« {len(grid_results_df.columns)} ä¸ªå­—æ®µ")
print(f"   å­—æ®µåŒ…æ‹¬: id, lon, lat, {len(shape_features)} ä¸ªå½¢æ€ç‰¹å¾, {len(target_cols)} ä¸ªç›®æ ‡å˜é‡, 3ä¸ªé¢„æµ‹å€¼")

# =========================
# 8. å¯è§†åŒ–
# =========================
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(df["lon"], df["lat"], c=df["pred_in"], cmap="viridis", s=10)
plt.colorbar(label="Predicted Inflow")
plt.title("Improved GNN - Predicted Inflow Heatmap")

plt.subplot(1, 3, 2)
plt.scatter(df["lon"], df["lat"], c=df["pred_out"], cmap="viridis", s=10)
plt.colorbar(label="Predicted Outflow")
plt.title("Improved GNN - Predicted Outflow Heatmap")

plt.subplot(1, 3, 3)
plt.scatter(df["lon"], df["lat"], c=df["pred_between"], cmap="viridis", s=10)
plt.colorbar(label="Predicted Betweenness")
plt.title("Improved GNN - Predicted Betweenness Heatmap")

plt.tight_layout()
plt.savefig('regession/output/basic_prediction_heatmaps_play.png', dpi=300, bbox_inches='tight')

import pandas as pd
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np

import pandas as pd
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np

# è¯»å–é¢„æµ‹ç»“æœ
df = pd.read_csv('regession/output/play2019_0906.csv')

# è®¡ç®—RÂ²åˆ†æ•°
r2_in = r2_score(df['PLAY_D_IN'], df['pred_in'])
r2_out = r2_score(df['PLAY_O_OUT'], df['pred_out'])
r2_between = r2_score(df['PLAY__Betweenness'], df['pred_between'])

# è®¡ç®—MAE
mae_in = mean_absolute_error(df['PLAY_D_IN'], df['pred_in'])
mae_out = mean_absolute_error(df['PLAY_O_OUT'], df['pred_out'])
mae_between = mean_absolute_error(df['PLAY__Betweenness'], df['pred_between'])

# è®¡ç®—RMSE
rmse_in = np.sqrt(mean_squared_error(df['PLAY_D_IN'], df['pred_in']))
rmse_out = np.sqrt(mean_squared_error(df['PLAY_O_OUT'], df['pred_out']))
rmse_between = np.sqrt(mean_squared_error(df['PLAY__Betweenness'], df['pred_between']))

print("=== æ¨¡å‹æ€§èƒ½è¯„ä¼° ===")
print(f"RÂ² åˆ†æ•°:")
print(f"  æµå…¥é¢„æµ‹: {r2_in:.4f}")
print(f"  æµå‡ºé¢„æµ‹: {r2_out:.4f}")
print(f"  Betweennessé¢„æµ‹: {r2_between:.4f}")

print(f"\nMAE (å¹³å‡ç»å¯¹è¯¯å·®):")
print(f"  æµå…¥é¢„æµ‹: {mae_in:.2f}")
print(f"  æµå‡ºé¢„æµ‹: {mae_out:.2f}")
print(f"  Betweennessé¢„æµ‹: {mae_between:.2f}")

print(f"\nRMSE (å‡æ–¹æ ¹è¯¯å·®):")
print(f"  æµå…¥é¢„æµ‹: {rmse_in:.2f}")
print(f"  æµå‡ºé¢„æµ‹: {rmse_out:.2f}")
print(f"  Betweennessé¢„æµ‹: {rmse_between:.2f}")

print(f"\né¢„æµ‹å€¼ç»Ÿè®¡:")
print(f"  æµå…¥: æœ€å°å€¼={df['pred_in'].min():.2f}, æœ€å¤§å€¼={df['pred_in'].max():.2f}")
print(f"  æµå‡º: æœ€å°å€¼={df['pred_out'].min():.2f}, æœ€å¤§å€¼={df['pred_out'].max():.2f}")
print(f"  Betweenness: æœ€å°å€¼={df['pred_between'].min():.2f}, æœ€å¤§å€¼={df['pred_between'].max():.2f}")

print(f"\nçœŸå®å€¼ç»Ÿè®¡:")
print(f"  æµå…¥: æœ€å°å€¼={df['PLAY_D_IN'].min():.2f}, æœ€å¤§å€¼={df['PLAY_D_IN'].max():.2f}")
print(f"  æµå‡º: æœ€å°å€¼={df['PLAY_O_OUT'].min():.2f}, æœ€å¤§å€¼={df['PLAY_O_OUT'].max():.2f}")
print(f"  Betweenness: æœ€å°å€¼={df['PLAY__Betweenness'].min():.2f}, æœ€å¤§å€¼={df['PLAY__Betweenness'].max():.2f}")

# =========================
# 9. å‚æ•°/ç‰¹å¾é‡è¦æ€§ä¸å¯è§£é‡Šæ€§åˆ†æï¼ˆRF + SHAP + æƒé‡åˆ†æï¼‰
# =========================

# è¾“å‡ºç›®å½•
_output_dir = "regession/output"
os.makedirs(_output_dir, exist_ok=True)

_shape_features = [
   "building_ratio0", "railway_ratio1", "road_ratio2", "water_ratio3",
    "instru_ratio4", "green_ratio5", "square_ratio6", "undevelop_ratio7",
    "playground_ratio8", "LPI_Max", "PR_COUNTclass", "SHI_zhouchang",
    "RND_road", "Social", "Life", "Entertainment",
    "Production", "POI_density", "MAX_Elevation", "MEAN_Elevation",
]
_target_cols = ["PLAY_D_IN", "PLAY_O_OUT", "PLAY__Betweenness"]

def analyze_feature_importance_rf(X_np, Y_np):
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    for i, target_name in enumerate(_target_cols):
        rf = RandomForestRegressor(n_estimators=200, random_state=42)
        rf.fit(X_np, Y_np[:, i])
        importance = rf.feature_importances_
        indices = np.argsort(importance)[::-1]
        axes[i].bar(range(len(importance)), importance[indices])
        axes[i].set_xticks(range(len(importance)))
        axes[i].set_xticklabels([_shape_features[j] for j in indices], rotation=45, ha='right')
        axes[i].set_title(f'{target_name} - ç‰¹å¾é‡è¦æ€§(RF)')
        axes[i].set_ylabel('é‡è¦æ€§')
        # ä¿å­˜CSV
        df_imp = pd.DataFrame({
            'feature': _shape_features,
            'importance': importance
        }).sort_values('importance', ascending=False)
        df_imp.to_csv(os.path.join(_output_dir, f'feature_importance_rf_{target_name}.csv'), index=False)
    plt.tight_layout()
    plt.savefig(os.path.join(_output_dir, 'feature_importance_rf.png'), dpi=300, bbox_inches='tight')
    plt.close()

def analyze_shap_rf(X_np, Y_np):
    if not _shap_available:
        print('âš ï¸ shap æœªå®‰è£…ï¼Œè·³è¿‡ SHAP åˆ†æã€‚å¯è¿è¡Œ: pip install shap')
        return
    for i, target_name in enumerate(_target_cols):
        rf = RandomForestRegressor(n_estimators=200, random_state=42)
        rf.fit(X_np, Y_np[:, i])
        explainer = shap.TreeExplainer(rf)
        sample_size = min(300, len(X_np))
        sample_idx = np.random.choice(len(X_np), sample_size, replace=False)
        X_sample = X_np[sample_idx]
        shap_values = explainer.shap_values(X_sample)
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values, X_sample, feature_names=_shape_features, show=False, plot_type='bar')
        plt.title(f'{target_name} - SHAPç‰¹å¾é‡è¦æ€§')
        plt.tight_layout()
        plt.savefig(os.path.join(_output_dir, f'shap_summary_{target_name}.png'), dpi=300, bbox_inches='tight')
        plt.close()

def analyze_model_weights_gnn(model_inst):
    try:
        conv1_weights = model_inst.conv1.weight.data.numpy()
    except Exception:
        print('âš ï¸ æ— æ³•è®¿é—®ç¬¬ä¸€å±‚GCNæƒé‡ï¼Œè·³è¿‡æƒé‡åˆ†æã€‚')
        return
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    axes[0].hist(conv1_weights.flatten(), bins=60, alpha=0.8, color='steelblue')
    axes[0].set_title('ç¬¬ä¸€å±‚GCNæƒé‡åˆ†å¸ƒ')
    axes[0].set_xlabel('æƒé‡å€¼')
    axes[0].set_ylabel('é¢‘æ¬¡')
    feature_importance = np.mean(np.abs(conv1_weights), axis=0)
    idx = np.argsort(feature_importance)[::-1]
    axes[1].bar(range(len(feature_importance)), feature_importance[idx])
    axes[1].set_xticks(range(len(feature_importance)))
    axes[1].set_xticklabels([_shape_features[j] for j in idx], rotation=45, ha='right')
    axes[1].set_title('åŸºäºç¬¬ä¸€å±‚æƒé‡çš„ç‰¹å¾é‡è¦æ€§')
    axes[1].set_ylabel('å¹³å‡ç»å¯¹æƒé‡')
    plt.tight_layout()
    plt.savefig(os.path.join(_output_dir, 'gnn_weight_based_importance_play.png'), dpi=300, bbox_inches='tight')
    plt.close()
    # ä¿å­˜æ•°å€¼
    pd.DataFrame({
        'feature': _shape_features,
        'abs_weight_mean': feature_importance
    }).sort_values('abs_weight_mean', ascending=False).to_csv(
        os.path.join(_output_dir, 'gnn_weight_based_importance_play.csv'), index=False
    )

# =========================
# 10. æ¯ä¸ªç½‘æ ¼çš„ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
# =========================
def analyze_per_grid_feature_importance(X_np, Y_np, coords, df_original):
    """
    è®¡ç®—æ¯ä¸ªç½‘æ ¼çš„æ¯ä¸ªç‰¹å¾é‡è¦æ€§ï¼Œå¹¶è¾“å‡ºåˆ°CSV
    """
    print('\nå¼€å§‹è®¡ç®—æ¯ä¸ªç½‘æ ¼çš„ç‰¹å¾é‡è¦æ€§...')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    grid_importance_dir = os.path.join(_output_dir, 'grid_feature_importance')
    os.makedirs(grid_importance_dir, exist_ok=True)
    
    # ä¸ºæ¯ä¸ªç›®æ ‡å˜é‡è®¡ç®—ç‰¹å¾é‡è¦æ€§
    for i, target_name in enumerate(_target_cols):
        print(f'æ­£åœ¨è®¡ç®— {target_name} çš„æ¯ä¸ªç½‘æ ¼ç‰¹å¾é‡è¦æ€§...')
        
        # ä½¿ç”¨éšæœºæ£®æ—è®¡ç®—SHAPå€¼
        rf = RandomForestRegressor(n_estimators=200, random_state=42)
        rf.fit(X_np, Y_np[:, i])
        
        # è®¡ç®—SHAPå€¼
        if _shap_available:
            explainer = shap.TreeExplainer(rf)
            shap_values = explainer.shap_values(X_np)
            
            # åˆ›å»ºæ¯ä¸ªç½‘æ ¼çš„ç‰¹å¾é‡è¦æ€§DataFrame
            grid_importance_data = []
            
            for grid_idx in range(len(X_np)):
                grid_data = {
                    'grid_id': df_original.iloc[grid_idx]['id2019'],  # ä½¿ç”¨åŸå§‹æ•°æ®ä¸­çš„id2019
                    'lon': coords[grid_idx, 0],
                    'lat': coords[grid_idx, 1],
                    'actual_value': Y_np[grid_idx, i],
                    'predicted_value': rf.predict(X_np[grid_idx:grid_idx+1])[0]
                }
                
                # æ·»åŠ æ¯ä¸ªç‰¹å¾çš„SHAPå€¼ï¼ˆç‰¹å¾é‡è¦æ€§ï¼‰
                for feature_idx, feature_name in enumerate(_shape_features):
                    grid_data[f'{feature_name}_shap_value'] = shap_values[grid_idx, feature_idx]
                    grid_data[f'{feature_name}_feature_value'] = X_np[grid_idx, feature_idx]
                
                grid_importance_data.append(grid_data)
            
            # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
            grid_df = pd.DataFrame(grid_importance_data)
            output_file = os.path.join(grid_importance_dir, f'grid_feature_importance_{target_name}.csv')
            grid_df.to_csv(output_file, index=False)
            
            print(f'âœ… {target_name} çš„æ¯ä¸ªç½‘æ ¼ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ°: {output_file}')
            
            # åˆ›å»ºç‰¹å¾é‡è¦æ€§æ±‡æ€»ç»Ÿè®¡
            summary_data = []
            for feature_name in _shape_features:
                shap_col = f'{feature_name}_shap_value'
                summary_data.append({
                    'feature_name': feature_name,
                    'mean_shap_value': grid_df[shap_col].mean(),
                    'std_shap_value': grid_df[shap_col].std(),
                    'abs_mean_shap_value': grid_df[shap_col].abs().mean(),
                    'max_shap_value': grid_df[shap_col].max(),
                    'min_shap_value': grid_df[shap_col].min(),
                    'feature_importance_rank': grid_df[shap_col].abs().mean()
                })
            
            summary_df = pd.DataFrame(summary_data)
            summary_df = summary_df.sort_values('feature_importance_rank', ascending=False)
            summary_file = os.path.join(grid_importance_dir, f'feature_importance_summary_{target_name}.csv')
            summary_df.to_csv(summary_file, index=False)
            
            print(f'âœ… {target_name} çš„ç‰¹å¾é‡è¦æ€§æ±‡æ€»å·²ä¿å­˜åˆ°: {summary_file}')
            
        else:
            print(f'âš ï¸ SHAP æœªå®‰è£…ï¼Œæ— æ³•è®¡ç®— {target_name} çš„æ¯ä¸ªç½‘æ ¼ç‰¹å¾é‡è¦æ€§')
    
    # åˆ›å»ºæ‰€æœ‰ç½‘æ ¼çš„ç»¼åˆç‰¹å¾é‡è¦æ€§è¡¨
    if _shap_available:
        print('\nåˆ›å»ºç»¼åˆç‰¹å¾é‡è¦æ€§è¡¨...')
        
        # åˆå¹¶æ‰€æœ‰ç›®æ ‡å˜é‡çš„ç‰¹å¾é‡è¦æ€§
        all_features_data = []
        
        for grid_idx in range(len(X_np)):
            grid_data = {
                'grid_id': df_original.iloc[grid_idx]['id2019'],  # ä½¿ç”¨åŸå§‹æ•°æ®ä¸­çš„id2019
                'lon': coords[grid_idx, 0],
                'lat': coords[grid_idx, 1]
            }
            
            # æ·»åŠ æ¯ä¸ªç›®æ ‡å˜é‡çš„å®é™…å€¼å’Œé¢„æµ‹å€¼
            for i, target_name in enumerate(_target_cols):
                grid_data[f'{target_name}_actual'] = Y_np[grid_idx, i]
                rf = RandomForestRegressor(n_estimators=200, random_state=42)
                rf.fit(X_np, Y_np[:, i])
                grid_data[f'{target_name}_predicted'] = rf.predict(X_np[grid_idx:grid_idx+1])[0]
            
            # æ·»åŠ æ¯ä¸ªç‰¹å¾çš„åŸå§‹å€¼
            for feature_idx, feature_name in enumerate(_shape_features):
                grid_data[f'{feature_name}_value'] = X_np[grid_idx, feature_idx]
            
            # æ·»åŠ æ¯ä¸ªç‰¹å¾å¯¹æ¯ä¸ªç›®æ ‡å˜é‡çš„SHAPå€¼
            for i, target_name in enumerate(_target_cols):
                rf = RandomForestRegressor(n_estimators=200, random_state=42)
                rf.fit(X_np, Y_np[:, i])
                explainer = shap.TreeExplainer(rf)
                shap_values = explainer.shap_values(X_np[grid_idx:grid_idx+1])
                
                for feature_idx, feature_name in enumerate(_shape_features):
                    grid_data[f'{feature_name}_shap_{target_name}'] = shap_values[0, feature_idx]
            
            all_features_data.append(grid_data)
        
        # ä¿å­˜ç»¼åˆè¡¨
        comprehensive_df = pd.DataFrame(all_features_data)
        comprehensive_file = os.path.join(grid_importance_dir, 'comprehensive_grid_feature_importance.csv')
        comprehensive_df.to_csv(comprehensive_file, index=False)
        
        print(f'âœ… ç»¼åˆç‰¹å¾é‡è¦æ€§è¡¨å·²ä¿å­˜åˆ°: {comprehensive_file}')
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾æ•°æ®
        print('\nåˆ›å»ºç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾æ•°æ®...')
        
        for i, target_name in enumerate(_target_cols):
            rf = RandomForestRegressor(n_estimators=200, random_state=42)
            rf.fit(X_np, Y_np[:, i])
            explainer = shap.TreeExplainer(rf)
            shap_values = explainer.shap_values(X_np)
            
            # åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®
            heatmap_data = []
            for grid_idx in range(len(X_np)):
                for feature_idx, feature_name in enumerate(_shape_features):
                    heatmap_data.append({
                        'grid_id': df_original.iloc[grid_idx]['id2019'],  # ä½¿ç”¨åŸå§‹æ•°æ®ä¸­çš„id2019
                        'lon': coords[grid_idx, 0],
                        'lat': coords[grid_idx, 1],
                        'feature_name': feature_name,
                        'shap_value': shap_values[grid_idx, feature_idx],
                        'abs_shap_value': abs(shap_values[grid_idx, feature_idx]),
                        'feature_value': X_np[grid_idx, feature_idx]
                    })
            
            heatmap_df = pd.DataFrame(heatmap_data)
            heatmap_file = os.path.join(grid_importance_dir, f'feature_importance_heatmap_data_{target_name}.csv')
            heatmap_df.to_csv(heatmap_file, index=False)
            
            print(f'âœ… {target_name} çš„ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾æ•°æ®å·²ä¿å­˜åˆ°: {heatmap_file}')

# æ‰§è¡Œåˆ†æ
print('\nå¼€å§‹å‚æ•°/ç‰¹å¾é‡è¦æ€§ä¸å¯è§£é‡Šæ€§åˆ†æ...')
X_np = X_processed  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
Y_np = Y_original
analyze_feature_importance_rf(X_np, Y_np)
analyze_shap_rf(X_np, Y_np)
analyze_model_weights_gnn(model)

# æ‰§è¡Œæ¯ä¸ªç½‘æ ¼çš„ç‰¹å¾é‡è¦æ€§åˆ†æ
analyze_per_grid_feature_importance(X_np, Y_np, coords, df)

print('âœ… åˆ†æå®Œæˆã€‚ç»“æœå·²ä¿å­˜è‡³ regession/output/ ç›®å½•ã€‚')
